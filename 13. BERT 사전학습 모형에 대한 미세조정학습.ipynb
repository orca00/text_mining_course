{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMw+e5/yMv73UulexGJhrcO"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"n9V4Eoqvs9no","executionInfo":{"status":"ok","timestamp":1678540030697,"user_tz":-540,"elapsed":4850,"user":{"displayName":"김학균","userId":"03507288958997861365"}},"outputId":"eb16a5c0-c737-4206-ebdb-37745af39613"},"outputs":[{"output_type":"stream","name":"stdout","text":["What a beautiful day! 토큰화 결과: ['what', 'a', 'beautiful', 'day', '!']\n","Nvidia Titan XP has 12GB of VRAM 토큰화 결과: ['n', '##vid', '##ia', 'titan', 'xp', 'has', '12', '##gb', 'of', 'vr', '##am']\n"]}],"source":["from transformers import BertTokenizer\n","\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","\n","sentence1 = \"What a beautiful day!\"\n","sentence2 = \"Nvidia Titan XP has 12GB of VRAM\"\n","\n","print(sentence1, '토큰화 결과:', tokenizer.tokenize(sentence1))\n","print(sentence2, '토큰화 결과:', tokenizer.tokenize(sentence2))"]},{"cell_type":"code","source":["inputs = tokenizer([sentence1, sentence2], padding=True)\n","print('BERT 입력:', inputs)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LwhV3XjVuHS3","executionInfo":{"status":"ok","timestamp":1678540030697,"user_tz":-540,"elapsed":10,"user":{"displayName":"김학균","userId":"03507288958997861365"}},"outputId":"e4b2e062-e635-463d-b179-2fd7b7b14faa"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["BERT 입력: {'input_ids': [[101, 2054, 1037, 3376, 2154, 999, 102, 0, 0, 0, 0, 0, 0], [101, 1050, 17258, 2401, 16537, 26726, 2038, 2260, 18259, 1997, 27830, 3286, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"]}]},{"cell_type":"code","source":["inputs = tokenizer(sentence1, sentence2, padding=True)\n","print('두 문장 시퀀스에 대한 BERT 입력:', inputs)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4Jq2x4i-uWC1","executionInfo":{"status":"ok","timestamp":1678540030697,"user_tz":-540,"elapsed":7,"user":{"displayName":"김학균","userId":"03507288958997861365"}},"outputId":"2044832d-986d-4ed3-c128-dba2c7d650b5"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["두 문장 시퀀스에 대한 BERT 입력: {'input_ids': [101, 2054, 1037, 3376, 2154, 999, 102, 1050, 17258, 2401, 16537, 26726, 2038, 2260, 18259, 1997, 27830, 3286, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"]}]},{"cell_type":"code","source":["import nltk\n","from nltk.corpus import movie_reviews\n","\n","from sklearn.model_selection import train_test_split\n","import numpy as numpy\n","\n","nltk.download('movie_reviews')\n","fileids = movie_reviews.fileids()\n","\n","reviews = [movie_reviews.raw(fileid) for fileid in fileids]\n","categories = [movie_reviews.categories(fileid)[0] for fileid in fileids]\n","\n","label_dict = {'pos':1, 'neg':0}\n","y = [label_dict[c] for c in categories]\n","\n","X_train, X_test, y_train, y_test = train_test_split(\n","    reviews, y, test_size=0.2, random_state=7\n",")\n","\n","print('Train set count:', len(X_train))\n","print('Test set count:', len(X_test))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UBcPOcy2uidR","executionInfo":{"status":"ok","timestamp":1678540032073,"user_tz":-540,"elapsed":1380,"user":{"displayName":"김학균","userId":"03507288958997861365"}},"outputId":"010e3559-3e87-4250-94ab-0cffb574362d"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n","[nltk_data]   Package movie_reviews is already up-to-date!\n"]},{"output_type":"stream","name":"stdout","text":["Train set count: 1600\n","Test set count: 400\n"]}]},{"cell_type":"code","source":["from transformers import BertTokenizerFast, BertForSequenceClassification\n","\n","tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n","model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n","\n","train_input = tokenizer(X_train, truncation=True, padding=True, return_tensors=\"pt\")\n","test_input = tokenizer(X_test, truncation=True, padding=True, return_tensors=\"pt\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KGcEvXvyvKHp","executionInfo":{"status":"ok","timestamp":1678540049069,"user_tz":-540,"elapsed":16998,"user":{"displayName":"김학균","userId":"03507288958997861365"}},"outputId":"6e7ee42c-706a-45db-ac1c-2d407a20f1c0"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}]},{"cell_type":"code","source":["import torch\n","\n","class OurDataset(torch.utils.data.Dataset):\n","    def __init__(self, inputs, labels):\n","        self.inputs = inputs\n","        self.labels = labels\n","\n","    def __getitem__(self, idx):\n","        item = {key: torch.tensor(val[idx]) for key, val in self.inputs.items()}\n","        item['labels'] = torch.tensor(self.labels[idx])\n","        return item\n","\n","    def __len__(self):\n","        return len(self.labels)\n","\n","train_dataset = OurDataset(train_input, y_train)\n","test_dataset = OurDataset(test_input, y_test)"],"metadata":{"id":"57tjR6D3vwDF","executionInfo":{"status":"ok","timestamp":1678540115207,"user_tz":-540,"elapsed":274,"user":{"displayName":"김학균","userId":"03507288958997861365"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["from datasets import load_metric\n","\n","metric = load_metric(\"accuracy\")\n","\n","def compute_metrics(eval_pred):\n","    logits, labels = eval_pred\n","    predictions = np.argmax(logits, axis=-1)\n","    return metric.compute(predictions=predictions, references=labels)"],"metadata":{"id":"STr5K4gWwpcD","executionInfo":{"status":"ok","timestamp":1678540121533,"user_tz":-540,"elapsed":722,"user":{"displayName":"김학균","userId":"03507288958997861365"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["from transformers import Trainer, TrainingArguments\n","\n","training_args = TrainingArguments(\n","    output_dir='./results',         \n","    num_train_epochs=2,            \n","    per_device_train_batch_size=8,  \n","    per_device_eval_batch_size=16,   \n",")\n","\n","trainer = Trainer(\n","    model=model,                    \n","    args=training_args,              \n","    train_dataset=train_dataset,  \n","    compute_metrics=compute_metrics,\n",")\n","\n","# 미세조정학습 실행\n","trainer.train()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":353},"id":"16C0S6KfxHtk","outputId":"5e68edfd-d65e-4f37-e006-94099ed7ba57"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","***** Running training *****\n","  Num examples = 1600\n","  Num Epochs = 2\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 400\n","  Number of trainable parameters = 109483778\n","<ipython-input-12-07e699bc1a98>:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  item = {key: torch.tensor(val[idx]) for key, val in self.inputs.items()}\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='64' max='400' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 64/400 49:28 < 4:28:08, 0.02 it/s, Epoch 0.32/2]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","  </tbody>\n","</table><p>"]},"metadata":{}}]},{"cell_type":"code","source":["trainer.evaluate(eval_dataset=test_dataset)"],"metadata":{"id":"1Jle3jkwyj9E"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["del model\n","del trainer\n","torch.cuda.empty_cache()"],"metadata":{"id":"olu_E2C1yqQS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torch.utils.data import DataLoader\n","\n","train_loader = DataLoader(train_dataset, shuffle=True, batch_size=8)"],"metadata":{"id":"TT3QDvZfyq09"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import BertModel\n","\n","bert_model = BertModel.from_pretrained('bert-base-uncased')"],"metadata":{"id":"gmq9xaPEysZJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class MyModel(torch.nn.Module):\n","    def __init__(self, pretrained_model, token_size, num_labels): \n","        super(MyModel, self).__init__()\n","        self.token_size = token_size\n","        self.num_labels = num_labels\n","        self.pretrained_model = pretrained_model\n","\n","        self.classifier = torch.nn.Linear(self.token_size, self.num_labels)\n","\n","    def forward(self, inputs):\n","        outputs = self.pretrained_model(**inputs)\n","        bert_clf_token = outputs.last_hidden_state[:,0,:]\n","        \n","        return self.classifier(bert_clf_token)\n","\n","model = MyModel(bert_model, num_labels=2, token_size=bert_model.config.hidden_size) "],"metadata":{"id":"XYHxXTKcyt69"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import AdamW\n","import torch.nn.functional as F\n","import time\n","\n","device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","\n","model.to(device)\n","model.train()     \n","\n","optim = AdamW(model.parameters(), lr=5e-5) \n","criterion = torch.nn.CrossEntropyLoss()\n","\n","start = time.time() \n","num_epochs = 4      \n","for epoch in range(num_epochs):\n","    total_epoch_loss = 0 \n","    for step, batch in enumerate(train_loader):\n","        optim.zero_grad()  \n","        inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'} \n","        labels = batch['labels'].to(device) \n","        outputs = model(inputs)\n","        loss = criterion(outputs, F.one_hot(labels, num_classes=2).float())\n","\n","        if (step+1) % 100 == 0:\n","            elapsed = time.time() - start\n","            print('Epoch %d, batch %d, elapsed time: %.2f, loss: %.4f' % (epoch+1, step+1, elapsed, loss))\n","        total_epoch_loss += loss\n","        loss.backward() \n","        optim.step()   \n","    avg_epoch_loss = total_epoch_loss / len(train_loader)\n","    print('Average loss for epoch %d: %.4f' % (epoch+1, avg_epoch_loss))\n"],"metadata":{"id":"GIdU9SfVyymc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from datasets import load_metric\n","\n","test_loader = DataLoader(test_dataset, batch_size=16)\n","\n","metric= load_metric(\"accuracy\")\n","model.eval()\n","for batch in test_loader:\n","    inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'}\n","    labels = batch['labels'].to(device)\n","    \n","    with torch.no_grad():\n","        outputs = model(inputs)\n","\n","    predictions = torch.argmax(outputs, dim=-1)\n","    metric.add_batch(predictions=predictions, references=labels)\n","\n","metric.compute()"],"metadata":{"id":"F1xrNFMGy7av"},"execution_count":null,"outputs":[]}]}